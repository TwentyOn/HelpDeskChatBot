import json
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.snowball import RussianStemmer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import jaccard_score
from string import punctuation
from sklearn.preprocessing import normalize
from scipy.sparse import csr_matrix

def get_answer(request):

    # request = '–ö–∞–∫ –º–Ω–µ –Ω–∞–π—Ç–∏ –æ–±—â–µ–∂–∏—Ç–∏–µ –≥–æ—Ä–Ω—è–∫'
    # request = input()

    vectorizer = TfidfVectorizer()
    stemmer = RussianStemmer()
    stop_word = set(stopwords.words('russian'))


    def preprocess(text):
        text = text.lower()
        text = ''.join([p for p in text if p not in punctuation])
        tokens = word_tokenize(text)
        tokens = [stemmer.stem(word) for word in tokens if word not in stop_word]
        return ' '.join(tokens)


    with open('data.json', encoding='utf-8') as file:
        txt = json.load(file)
    data = tuple(t['–î–æ–ª–∂–Ω–æ—Å—Ç—å'] for t in txt)
    preprocess_data = list(preprocess(new_data) for new_data in data)

    tfidf_matrix = vectorizer.fit_transform([preprocess(request)] + preprocess_data)  # –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ tfidf
    sparse_tfidf_matrix = csr_matrix(tfidf_matrix)  # –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É
    tfidf_matrix_normalized = normalize(tfidf_matrix, norm='l2', axis=1)  # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –ø–æ –¥–ª–∏–Ω–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞


    def jaccard_similarity():  # –º–µ—Ç—Ä–∏–∫–∞ –ñ–∞–∫–∫–∞—Ä–¥–∞
        vectorizer = TfidfVectorizer(binary=True)  # binary=True –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ
        tfidf_matrix = vectorizer.fit_transform([preprocess(request)] + preprocess_data)
        binary_matrix = tfidf_matrix.toarray()

        num_docs = binary_matrix.shape[0]
        jaccard_matrix = np.zeros((num_docs, num_docs))

        for i in range(num_docs):
            for j in range(i, num_docs):  # –°–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞
                intersection = np.sum(np.minimum(binary_matrix[i], binary_matrix[j]))
                union = np.sum(np.maximum(binary_matrix[i], binary_matrix[j]))
                jaccard_matrix[i, j] = intersection / union
                jaccard_matrix[j, i] = jaccard_matrix[i, j]

        return jaccard_matrix


    def euclidean_distance(v1, v2):
        return np.linalg.norm(v1 - v2)


    def euclidean_distance_matrix():  # –º–µ—Ç—Ä–∏–∫–∞ –ï–≤–∫–ª–∏–¥–æ–≤–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É –≤ –æ–±—ã—á–Ω—É—é –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        tfidf_matrix = tfidf_matrix_normalized.toarray()

        num_docs = tfidf_matrix.shape[0]  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–≤–∫–ª—é—á–∞—è –∑–∞–ø—Ä–æ—Å)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ï–≤–∫–ª–∏–¥–æ–≤—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
        euclidean_matrix = np.zeros((num_docs, num_docs))

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –º–∞—Ç—Ä–∏—Ü—É –ï–≤–∫–ª–∏–¥–æ–≤—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
        for i in range(num_docs):
            for j in range(i, num_docs):  # —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞
                # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ï–≤–∫–ª–∏–¥–æ–≤–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏ i –∏ j
                distance = euclidean_distance(tfidf_matrix[i], tfidf_matrix[j])
                euclidean_matrix[i, j] = distance
                euclidean_matrix[j, i] = distance

        return euclidean_matrix


    def cos_similarity():
        cos_similatity_matrix = cosine_similarity(tfidf_matrix_normalized)
        return cos_similatity_matrix

    cosine_similarity_matrix = cos_similarity()
    jaccard_matrix = jaccard_similarity()
    euclid_matrix = euclidean_distance_matrix()

    result = []

    for ind, similarity in enumerate(zip(cosine_similarity_matrix[0], jaccard_matrix[0], euclid_matrix[0])):
        if 0 < similarity[0] < 1:
            similaritys = list(similarity)
            similaritys[2] = 1 / (1 + similarity[2])  # –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ï–≤–∫–ª–∏–¥–æ–≤–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –≤ –±–∞–ª–ª –ø–æ–¥–æ–±–∏—è
            result.append(
                (tuple(similaritys), data[ind - 1], tuple(t['–§–ò–û'] for t in txt)[ind - 1],
                 tuple(t['–¢–µ–ª–µ—Ñ–æ–Ω'] for t in txt)[ind - 1]))

    sims = ['- –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ:', '- –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å—Ö–æ–¥—Å—Ç–≤–∞ –ñ–∞–∫–∫–∞—Ä–¥–∞:', '- –±–∞–ª–ª –ø–æ–¥–æ–±–∏—è –ø–æ –ï–≤–∫–ª–∏–¥–æ–≤—É —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é:']

    if not result:
        return '–ü–æ–ø—Ä–æ–±—É–π —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø–æ-–¥—Ä—É–≥–æ–º—É'
    else:
        out = '–í–æ–∑–º–æ–∂–Ω–æ –≤–∞–º –ø–æ–º–æ–≥—É—Ç:\n'
        for ans in result:
            if ans[0][0] > 0.20:
                out += f'üë§ {ans[2]}\n'
                out += f'üíº {ans[1]}\n'
                out += f'üìû {ans[3].replace(" ", "").replace("-", "")}\n'
                out += '\n–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã —Å—Ö–æ–¥—Å—Ç–≤–∞:\n'
                for i in zip(sims, ans[0]):
                    out += f'{i[0]} {i[1]}\n'
                out += '\n'

        return out